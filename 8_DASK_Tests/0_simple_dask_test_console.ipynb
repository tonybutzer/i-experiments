{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45297bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list | grep dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2893b1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-722d94f4-1cb7-11ec-9715-0a0c875286eb</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://10.12.71.82:8080/status\" target=\"_blank\">http://10.12.71.82:8080/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">0f6dd7d7</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://10.12.71.82:8080/status\" target=\"_blank\">http://10.12.71.82:8080/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 1\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 4\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 1.86 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> False</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-0472d819-b2a5-4261-ac51-8a56ce094210</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> inproc://10.12.71.82/22293/1\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 1\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://10.12.71.82:8080/status\" target=\"_blank\">http://10.12.71.82:8080/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 4\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 1.86 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> inproc://10.12.71.82/22293/3\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 4\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://10.12.71.82:37405/status\" target=\"_blank\">http://10.12.71.82:37405/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 1.86 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> None\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /home/ec2-user/opt/i-experiments/8_DASK_Tests/dask-worker-space/worker-lfg_gn78\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'inproc://10.12.71.82/22293/1' processes=1 threads=4, memory=1.86 GiB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(processes=False, threads_per_worker=4,\n",
    "                n_workers=1, memory_limit='2GB', dashboard_address=':8080')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47881d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Client in module distributed.client:\n",
      "\n",
      "class Client(builtins.object)\n",
      " |  Client(address=None, loop=None, timeout='__no_default__', set_as_default=True, scheduler_file=None, security=None, asynchronous=False, name=None, heartbeat_interval=None, serializers=None, deserializers=None, extensions=[<class 'distributed.pubsub.PubSubClientExtension'>], direct_to_workers=None, connection_limit=512, **kwargs)\n",
      " |  \n",
      " |  Connect to and submit computation to a Dask cluster\n",
      " |  \n",
      " |  The Client connects users to a Dask cluster.  It provides an asynchronous\n",
      " |  user interface around functions and futures.  This class resembles\n",
      " |  executors in ``concurrent.futures`` but also allows ``Future`` objects\n",
      " |  within ``submit/map`` calls.  When a Client is instantiated it takes over\n",
      " |  all ``dask.compute`` and ``dask.persist`` calls by default.\n",
      " |  \n",
      " |  It is also common to create a Client without specifying the scheduler\n",
      " |  address , like ``Client()``.  In this case the Client creates a\n",
      " |  :class:`LocalCluster` in the background and connects to that.  Any extra\n",
      " |  keywords are passed from Client to LocalCluster in this case.  See the\n",
      " |  LocalCluster documentation for more information.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  address: string, or Cluster\n",
      " |      This can be the address of a ``Scheduler`` server like a string\n",
      " |      ``'127.0.0.1:8786'`` or a cluster object like ``LocalCluster()``\n",
      " |  timeout: int\n",
      " |      Timeout duration for initial connection to the scheduler\n",
      " |  set_as_default: bool (True)\n",
      " |      Use this Client as the global dask scheduler\n",
      " |  scheduler_file: string (optional)\n",
      " |      Path to a file with scheduler information if available\n",
      " |  security: Security or bool, optional\n",
      " |      Optional security information. If creating a local cluster can also\n",
      " |      pass in ``True``, in which case temporary self-signed credentials will\n",
      " |      be created automatically.\n",
      " |  asynchronous: bool (False by default)\n",
      " |      Set to True if using this client within async/await functions or within\n",
      " |      Tornado gen.coroutines.  Otherwise this should remain False for normal\n",
      " |      use.\n",
      " |  name: string (optional)\n",
      " |      Gives the client a name that will be included in logs generated on\n",
      " |      the scheduler for matters relating to this client\n",
      " |  direct_to_workers: bool (optional)\n",
      " |      Whether or not to connect directly to the workers, or to ask\n",
      " |      the scheduler to serve as intermediary.\n",
      " |  heartbeat_interval: int\n",
      " |      Time in milliseconds between heartbeats to scheduler\n",
      " |  **kwargs:\n",
      " |      If you do not pass a scheduler address, Client will create a\n",
      " |      ``LocalCluster`` object, passing any extra keyword arguments.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Provide cluster's scheduler node address on initialization:\n",
      " |  \n",
      " |  >>> client = Client('127.0.0.1:8786')  # doctest: +SKIP\n",
      " |  \n",
      " |  Use ``submit`` method to send individual computations to the cluster\n",
      " |  \n",
      " |  >>> a = client.submit(add, 1, 2)  # doctest: +SKIP\n",
      " |  >>> b = client.submit(add, 10, 20)  # doctest: +SKIP\n",
      " |  \n",
      " |  Continue using submit or map on results to build up larger computations\n",
      " |  \n",
      " |  >>> c = client.submit(add, a, b)  # doctest: +SKIP\n",
      " |  \n",
      " |  Gather results with the ``gather`` method.\n",
      " |  \n",
      " |  >>> client.gather(c)  # doctest: +SKIP\n",
      " |  33\n",
      " |  \n",
      " |  You can also call Client with no arguments in order to create your own\n",
      " |  local cluster.\n",
      " |  \n",
      " |  >>> client = Client()  # makes your own local \"cluster\" # doctest: +SKIP\n",
      " |  \n",
      " |  Extra keywords will be passed directly to LocalCluster\n",
      " |  \n",
      " |  >>> client = Client(n_workers=2, threads_per_worker=4)  # doctest: +SKIP\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  distributed.scheduler.Scheduler: Internal scheduler\n",
      " |  distributed.LocalCluster:\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  async __aenter__(self)\n",
      " |  \n",
      " |  async __aexit__(self, typ, value, traceback)\n",
      " |  \n",
      " |  __await__(self)\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, type, value, traceback)\n",
      " |  \n",
      " |  __init__(self, address=None, loop=None, timeout='__no_default__', set_as_default=True, scheduler_file=None, security=None, asynchronous=False, name=None, heartbeat_interval=None, serializers=None, deserializers=None, extensions=[<class 'distributed.pubsub.PubSubClientExtension'>], direct_to_workers=None, connection_limit=512, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_current(self)\n",
      " |      Thread-local, Task-local context manager that causes the Client.current class\n",
      " |      method to return self. Any Future objects deserialized inside this context\n",
      " |      manager will be automatically attached to this Client.\n",
      " |  \n",
      " |  call_stack(self, futures=None, keys=None)\n",
      " |      The actively running call stack of all relevant keys\n",
      " |      \n",
      " |      You can specify data of interest either by providing futures or\n",
      " |      collections in the ``futures=`` keyword or a list of explicit keys in\n",
      " |      the ``keys=`` keyword.  If neither are provided then all call stacks\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list (optional)\n",
      " |          List of futures, defaults to all data\n",
      " |      keys : list (optional)\n",
      " |          List of key names, defaults to all data\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = dd.read_parquet(...).persist()  # doctest: +SKIP\n",
      " |      >>> client.call_stack(df)  # call on collections\n",
      " |      \n",
      " |      >>> client.call_stack()  # Or call with no arguments for all activity  # doctest: +SKIP\n",
      " |  \n",
      " |  cancel(self, futures, asynchronous=None, force=False)\n",
      " |      Cancel running futures\n",
      " |      \n",
      " |      This stops future tasks from being scheduled if they have not yet run\n",
      " |      and deletes them if they have already run.  After calling, this result\n",
      " |      and all dependent results will no longer be accessible\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list of Futures\n",
      " |      force : boolean (False)\n",
      " |          Cancel this future even if other clients desire it\n",
      " |  \n",
      " |  close(self, timeout='__no_default__')\n",
      " |      Close this client\n",
      " |      \n",
      " |      Clients will also close automatically when your Python session ends\n",
      " |      \n",
      " |      If you started a client without arguments like ``Client()`` then this\n",
      " |      will also close the local cluster that was started at the same time.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.restart\n",
      " |  \n",
      " |  compute(self, collections, sync=False, optimize_graph=True, workers=None, allow_other_workers=False, resources=None, retries=0, priority=0, fifo_timeout='60s', actors=None, traverse=True, **kwargs)\n",
      " |      Compute dask collections on cluster\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      collections : iterable of dask objects or single dask object\n",
      " |          Collections like dask.array or dataframe or dask.value objects\n",
      " |      sync : bool (optional)\n",
      " |          Returns Futures if False (default) or concrete values if True\n",
      " |      optimize_graph : bool\n",
      " |          Whether or not to optimize the underlying graphs\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker hostnames on which computations may be performed.\n",
      " |          Leave empty to default to all workers (common case)\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with `workers`. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if computing a result fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : timedelta str (defaults to '60s')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      traverse : bool (defaults to True)\n",
      " |          By default dask traverses builtin python collections looking for\n",
      " |          dask objects passed to ``compute``. For large collections this can\n",
      " |          be expensive. If none of the arguments contain any dask objects,\n",
      " |          set ``traverse=False`` to avoid doing this traversal.\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the `resources` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      actors : bool or dict (default None)\n",
      " |          Whether these tasks should exist on the worker as stateful actors.\n",
      " |          Specified on a global (True/False) or per-task (``{'x': True,\n",
      " |          'y': False}``) basis. See :doc:`actors` for additional details.\n",
      " |      **kwargs\n",
      " |          Options to pass to the graph optimize calls\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of Futures if input is a sequence, or a single future otherwise\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from dask import delayed\n",
      " |      >>> from operator import add\n",
      " |      >>> x = delayed(add)(1, 2)\n",
      " |      >>> y = delayed(add)(x, x)\n",
      " |      >>> xx, yy = client.compute([x, y])  # doctest: +SKIP\n",
      " |      >>> xx  # doctest: +SKIP\n",
      " |      <Future: status: finished, key: add-8f6e709446674bad78ea8aeecfee188e>\n",
      " |      >>> xx.result()  # doctest: +SKIP\n",
      " |      3\n",
      " |      >>> yy.result()  # doctest: +SKIP\n",
      " |      6\n",
      " |      \n",
      " |      Also support single arguments\n",
      " |      \n",
      " |      >>> xx = client.compute(x)  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.get : Normal synchronous dask.get function\n",
      " |  \n",
      " |  futures_of(self, futures)\n",
      " |  \n",
      " |  gather(self, futures, errors='raise', direct=None, asynchronous=None)\n",
      " |      Gather futures from distributed memory\n",
      " |      \n",
      " |      Accepts a future, nested container of futures, iterator, or queue.\n",
      " |      The return type will match the input type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : Collection of futures\n",
      " |          This can be a possibly nested collection of Future objects.\n",
      " |          Collections can be lists, sets, or dictionaries\n",
      " |      errors : string\n",
      " |          Either 'raise' or 'skip' if we should raise if a future has erred\n",
      " |          or skip its inclusion in the output collection\n",
      " |      direct : boolean\n",
      " |          Whether or not to connect directly to the workers, or to ask\n",
      " |          the scheduler to serve as intermediary.  This can also be set when\n",
      " |          creating the Client.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      results: a collection of the same type as the input, but now with\n",
      " |      gathered results rather than futures\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add  # doctest: +SKIP\n",
      " |      >>> c = Client('127.0.0.1:8787')  # doctest: +SKIP\n",
      " |      >>> x = c.submit(add, 1, 2)  # doctest: +SKIP\n",
      " |      >>> c.gather(x)  # doctest: +SKIP\n",
      " |      3\n",
      " |      >>> c.gather([x, [x], x])  # support lists and dicts # doctest: +SKIP\n",
      " |      [3, [3], 3]\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.scatter : Send data out to cluster\n",
      " |  \n",
      " |  get(self, dsk, keys, workers=None, allow_other_workers=None, resources=None, sync=True, asynchronous=None, direct=None, retries=None, priority=0, fifo_timeout='60s', actors=None, **kwargs)\n",
      " |      Compute dask graph\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dsk : dict\n",
      " |      keys : object, or nested lists of objects\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker addresses or hostnames on which computations may be\n",
      " |          performed. Leave empty to default to all workers (common case)\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with ``workers``. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if computing a result fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the ``resources`` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      sync : bool (optional)\n",
      " |          Returns Futures if False or concrete values if True (default).\n",
      " |      direct : bool\n",
      " |          Whether or not to connect directly to the workers, or to ask\n",
      " |          the scheduler to serve as intermediary.  This can also be set when\n",
      " |          creating the Client.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add  # doctest: +SKIP\n",
      " |      >>> c = Client('127.0.0.1:8787')  # doctest: +SKIP\n",
      " |      >>> c.get({'x': (add, 1, 2)}, 'x')  # doctest: +SKIP\n",
      " |      3\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.compute : Compute asynchronous collections\n",
      " |  \n",
      " |  get_dataset(self, name, default='_no_default_', **kwargs)\n",
      " |      Get named dataset from the scheduler if present.\n",
      " |      Return the default or raise a KeyError if not present.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : name of the dataset to retrieve\n",
      " |      default : optional, not set by default\n",
      " |          If set, do not raise a KeyError if the name is not present but return this default\n",
      " |      kwargs : dict\n",
      " |          additional arguments to _get_dataset\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.publish_dataset\n",
      " |      Client.list_datasets\n",
      " |  \n",
      " |  get_events(self, topic: str = None)\n",
      " |      Retrieve structured topic logs\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topic : str, optional\n",
      " |          Name of topic log to retrieve events for. If no ``topic`` is\n",
      " |          provided, then logs for all topics will be returned.\n",
      " |  \n",
      " |  get_executor(self, **kwargs)\n",
      " |      Return a concurrent.futures Executor for submitting tasks on this Client\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs\n",
      " |          Any submit()- or map()- compatible arguments, such as\n",
      " |          `workers` or `resources`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      An Executor object that's fully compatible with the concurrent.futures\n",
      " |      API.\n",
      " |  \n",
      " |  get_metadata(self, keys, default='__no_default__')\n",
      " |      Get arbitrary metadata from scheduler\n",
      " |      \n",
      " |      See set_metadata for the full docstring with examples\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : key or list\n",
      " |          Key to access.  If a list then gets within a nested collection\n",
      " |      default : optional\n",
      " |          If the key does not exist then return this value instead.\n",
      " |          If not provided then this raises a KeyError if the key is not\n",
      " |          present\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.set_metadata\n",
      " |  \n",
      " |  get_scheduler_logs(self, n=None)\n",
      " |      Get logs from scheduler\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of logs to retrive.  Maxes out at 10000 by default,\n",
      " |          configurable via the ``distributed.admin.log-length``\n",
      " |          configuration value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Logs in reversed order (newest first)\n",
      " |  \n",
      " |  get_task_stream(self, start=None, stop=None, count=None, plot=False, filename='task-stream.html', bokeh_resources=None)\n",
      " |      Get task stream data from scheduler\n",
      " |      \n",
      " |      This collects the data present in the diagnostic \"Task Stream\" plot on\n",
      " |      the dashboard.  It includes the start, stop, transfer, and\n",
      " |      deserialization time of every task for a particular duration.\n",
      " |      \n",
      " |      Note that the task stream diagnostic does not run by default.  You may\n",
      " |      wish to call this function once before you start work to ensure that\n",
      " |      things start recording, and then again after you have completed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : Number or string\n",
      " |          When you want to start recording\n",
      " |          If a number it should be the result of calling time()\n",
      " |          If a string then it should be a time difference before now,\n",
      " |          like '60s' or '500 ms'\n",
      " |      stop : Number or string\n",
      " |          When you want to stop recording\n",
      " |      count : int\n",
      " |          The number of desired records, ignored if both start and stop are\n",
      " |          specified\n",
      " |      plot : boolean, str\n",
      " |          If true then also return a Bokeh figure\n",
      " |          If plot == 'save' then save the figure to a file\n",
      " |      filename : str (optional)\n",
      " |          The filename to save to if you set ``plot='save'``\n",
      " |      bokeh_resources : bokeh.resources.Resources (optional)\n",
      " |          Specifies if the resource component is INLINE or CDN\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.get_task_stream()  # prime plugin if not already connected\n",
      " |      >>> x.compute()  # do some work\n",
      " |      >>> client.get_task_stream()\n",
      " |      [{'task': ...,\n",
      " |        'type': ...,\n",
      " |        'thread': ...,\n",
      " |        ...}]\n",
      " |      \n",
      " |      Pass the ``plot=True`` or ``plot='save'`` keywords to get back a Bokeh\n",
      " |      figure\n",
      " |      \n",
      " |      >>> data, figure = client.get_task_stream(plot='save', filename='myfile.html')\n",
      " |      \n",
      " |      Alternatively consider the context manager\n",
      " |      \n",
      " |      >>> from dask.distributed import get_task_stream\n",
      " |      >>> with get_task_stream() as ts:\n",
      " |      ...     x.compute()\n",
      " |      >>> ts.data\n",
      " |      [...]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      L: List[Dict]\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_task_stream : a context manager version of this method\n",
      " |  \n",
      " |  get_versions(self, check=False, packages=[])\n",
      " |      Return version info for the scheduler, all workers and myself\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      check : boolean, default False\n",
      " |          raise ValueError if all required & optional packages\n",
      " |          do not match\n",
      " |      packages : List[str]\n",
      " |          Extra package names to check\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.get_versions()  # doctest: +SKIP\n",
      " |      \n",
      " |      >>> c.get_versions(packages=['sklearn', 'geopandas'])  # doctest: +SKIP\n",
      " |  \n",
      " |  get_worker_logs(self, n=None, workers=None, nanny=False)\n",
      " |      Get logs from workers\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of logs to retrive.  Maxes out at 10000 by default,\n",
      " |          configurable via the ``distributed.admin.log-length``\n",
      " |          configuration value.\n",
      " |      workers : iterable\n",
      " |          List of worker addresses to retrieve.  Gets all workers by default.\n",
      " |      nanny : bool, default False\n",
      " |          Whether to get the logs from the workers (False) or the nannies (True). If\n",
      " |          specified, the addresses in `workers` should still be the worker addresses,\n",
      " |          not the nanny addresses.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dictionary mapping worker address to logs.\n",
      " |      Logs are returned in reversed order (newest first)\n",
      " |  \n",
      " |  has_what(self, workers=None, **kwargs)\n",
      " |      Which keys are held by which workers\n",
      " |      \n",
      " |      This returns the keys of the data that are held in each worker's\n",
      " |      memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list (optional)\n",
      " |          A list of worker addresses, defaults to all\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> wait([x, y, z])  # doctest: +SKIP\n",
      " |      >>> c.has_what()  # doctest: +SKIP\n",
      " |      {'192.168.1.141:46784': ['inc-1c8dd6be1c21646c71f76c16d09304ea',\n",
      " |                               'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b',\n",
      " |                               'inc-1e297fc27658d7b67b3a758f16bcf47a']}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |      Client.nthreads\n",
      " |      Client.processing\n",
      " |  \n",
      " |  list_datasets(self, **kwargs)\n",
      " |      List named datasets available on the scheduler\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.publish_dataset\n",
      " |      Client.get_dataset\n",
      " |  \n",
      " |  log_event(self, topic, msg)\n",
      " |      Log an event under a given topic\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      topic : str, list\n",
      " |          Name of the topic under which to log an event. To log the same\n",
      " |          event under multiple topics, pass a list of topic names.\n",
      " |      msg\n",
      " |          Event message to log. Note this must be msgpack serializable.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from time import time\n",
      " |      >>> client.log_event(\"current-time\", time())\n",
      " |  \n",
      " |  map(self, func, *iterables, key=None, workers=None, retries=None, resources=None, priority=0, allow_other_workers=False, fifo_timeout='100 ms', actor=False, actors=False, pure=None, batch_size=None, **kwargs)\n",
      " |      Map a function on a sequence of arguments\n",
      " |      \n",
      " |      Arguments can be normal objects or Futures\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Callable to be scheduled for execution. If ``func`` returns a coroutine, it\n",
      " |          will be run on the main event loop of a worker. Otherwise ``func`` will be\n",
      " |          run in a worker's task executor pool (see ``Worker.executors`` for more\n",
      " |          information.)\n",
      " |      iterables : Iterables\n",
      " |          List-like objects to map over.  They should have the same length.\n",
      " |      key : str, list\n",
      " |          Prefix for task names if string.  Explicit names if list.\n",
      " |      pure : bool (defaults to True)\n",
      " |          Whether or not the function is pure.  Set ``pure=False`` for\n",
      " |          impure functions like ``np.random.random``.\n",
      " |          See :ref:`pure functions` for more details.\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker hostnames on which computations may be performed.\n",
      " |          Leave empty to default to all workers (common case)\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with `workers`. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if a task fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : str timedelta (default '100ms')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the `resources` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      actor : bool (default False)\n",
      " |          Whether these tasks should exist on the worker as stateful actors.\n",
      " |          See :doc:`actors` for additional details.\n",
      " |      actors : bool (default False)\n",
      " |          Alias for `actor`\n",
      " |      batch_size : int, optional\n",
      " |          Submit tasks to the scheduler in batches of (at most) ``batch_size``.\n",
      " |          Larger batch sizes can be useful for very large ``iterables``,\n",
      " |          as the cluster can start processing tasks while later ones are\n",
      " |          submitted asynchronously.\n",
      " |      **kwargs : dict\n",
      " |          Extra keywords to send to the function.\n",
      " |          Large values will be included explicitly in the task graph.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> L = client.map(func, sequence)  # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List, iterator, or Queue of futures, depending on the type of the\n",
      " |      inputs.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.submit : Submit a single function\n",
      " |  \n",
      " |  nbytes(self, keys=None, summary=True, **kwargs)\n",
      " |      The bytes taken up by each key on the cluster\n",
      " |      \n",
      " |      This is as measured by ``sys.getsizeof`` which may not accurately\n",
      " |      reflect the true cost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list (optional)\n",
      " |          A list of keys, defaults to all keys\n",
      " |      summary : boolean, (optional)\n",
      " |          Summarize keys into key types\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> c.nbytes(summary=False)  # doctest: +SKIP\n",
      " |      {'inc-1c8dd6be1c21646c71f76c16d09304ea': 28,\n",
      " |       'inc-1e297fc27658d7b67b3a758f16bcf47a': 28,\n",
      " |       'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b': 28}\n",
      " |      \n",
      " |      >>> c.nbytes(summary=True)  # doctest: +SKIP\n",
      " |      {'inc': 84}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |  \n",
      " |  ncores = nthreads(self, workers=None, **kwargs)\n",
      " |  \n",
      " |  normalize_collection(self, collection)\n",
      " |      Replace collection's tasks by already existing futures if they exist\n",
      " |      \n",
      " |      This normalizes the tasks within a collections task graph against the\n",
      " |      known futures within the scheduler.  It returns a copy of the\n",
      " |      collection with a task graph that includes the overlapping futures.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> len(x.__dask_graph__())  # x is a dask collection with 100 tasks  # doctest: +SKIP\n",
      " |      100\n",
      " |      >>> set(client.futures).intersection(x.__dask_graph__())  # some overlap exists  # doctest: +SKIP\n",
      " |      10\n",
      " |      \n",
      " |      >>> x = client.normalize_collection(x)  # doctest: +SKIP\n",
      " |      >>> len(x.__dask_graph__())  # smaller computational graph  # doctest: +SKIP\n",
      " |      20\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.persist : trigger computation of collection's tasks\n",
      " |  \n",
      " |  nthreads(self, workers=None, **kwargs)\n",
      " |      The number of threads/cores available on each worker node\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list (optional)\n",
      " |          A list of workers that we care about specifically.\n",
      " |          Leave empty to receive information about all workers.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.threads()  # doctest: +SKIP\n",
      " |      {'192.168.1.141:46784': 8,\n",
      " |       '192.167.1.142:47548': 8,\n",
      " |       '192.167.1.143:47329': 8,\n",
      " |       '192.167.1.144:37297': 8}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |      Client.has_what\n",
      " |  \n",
      " |  persist(self, collections, optimize_graph=True, workers=None, allow_other_workers=None, resources=None, retries=None, priority=0, fifo_timeout='60s', actors=None, **kwargs)\n",
      " |      Persist dask collections on cluster\n",
      " |      \n",
      " |      Starts computation of the collection on the cluster in the background.\n",
      " |      Provides a new dask collection that is semantically identical to the\n",
      " |      previous one, but now based off of futures currently in execution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      collections : sequence or single dask object\n",
      " |          Collections like dask.array or dataframe or dask.value objects\n",
      " |      optimize_graph : bool\n",
      " |          Whether or not to optimize the underlying graphs\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker hostnames on which computations may be performed.\n",
      " |          Leave empty to default to all workers (common case)\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with `workers`. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if computing a result fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : timedelta str (defaults to '60s')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the `resources` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      actors : bool or dict (default None)\n",
      " |          Whether these tasks should exist on the worker as stateful actors.\n",
      " |          Specified on a global (True/False) or per-task (``{'x': True,\n",
      " |          'y': False}``) basis. See :doc:`actors` for additional details.\n",
      " |      **kwargs\n",
      " |          Options to pass to the graph optimize calls\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of collections, or single collection, depending on type of input.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> xx = client.persist(x)  # doctest: +SKIP\n",
      " |      >>> xx, yy = client.persist([x, y])  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.compute\n",
      " |  \n",
      " |  processing(self, workers=None)\n",
      " |      The tasks currently running on each worker\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list (optional)\n",
      " |          A list of worker addresses, defaults to all\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> c.processing()  # doctest: +SKIP\n",
      " |      {'192.168.1.141:46784': ['inc-1c8dd6be1c21646c71f76c16d09304ea',\n",
      " |                               'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b',\n",
      " |                               'inc-1e297fc27658d7b67b3a758f16bcf47a']}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.who_has\n",
      " |      Client.has_what\n",
      " |      Client.nthreads\n",
      " |  \n",
      " |  profile(self, key=None, start=None, stop=None, workers=None, merge_workers=True, plot=False, filename=None, server=False, scheduler=False)\n",
      " |      Collect statistical profiling information about recent work\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          Key prefix to select, this is typically a function name like 'inc'\n",
      " |          Leave as None to collect all data\n",
      " |      start : time\n",
      " |      stop : time\n",
      " |      workers : list\n",
      " |          List of workers to restrict profile information\n",
      " |      server : bool\n",
      " |          If true, return the profile of the worker's administrative thread\n",
      " |          rather than the worker threads.\n",
      " |          This is useful when profiling Dask itself, rather than user code.\n",
      " |      scheduler : bool\n",
      " |          If true, return the profile information from the scheduler's\n",
      " |          administrative thread rather than the workers.\n",
      " |          This is useful when profiling Dask's scheduling itself.\n",
      " |      plot : boolean or string\n",
      " |          Whether or not to return a plot object\n",
      " |      filename : str\n",
      " |          Filename to save the plot\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.profile()  # call on collections\n",
      " |      >>> client.profile(filename='dask-profile.html')  # save to html file\n",
      " |  \n",
      " |  publish_dataset(self, *args, **kwargs)\n",
      " |      Publish named datasets to scheduler\n",
      " |      \n",
      " |      This stores a named reference to a dask collection or list of futures\n",
      " |      on the scheduler.  These references are available to other Clients\n",
      " |      which can download the collection or futures with ``get_dataset``.\n",
      " |      \n",
      " |      Datasets are not immediately computed.  You may wish to call\n",
      " |      ``Client.persist`` prior to publishing a dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : list of objects to publish as name\n",
      " |      name : optional name of the dataset to publish\n",
      " |      override : bool (optional, default False)\n",
      " |          if true, override any already present dataset with the same name\n",
      " |      kwargs : dict\n",
      " |          named collections to publish on the scheduler\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Publishing client:\n",
      " |      \n",
      " |      >>> df = dd.read_csv('s3://...')  # doctest: +SKIP\n",
      " |      >>> df = c.persist(df) # doctest: +SKIP\n",
      " |      >>> c.publish_dataset(my_dataset=df)  # doctest: +SKIP\n",
      " |      \n",
      " |      Alternative invocation\n",
      " |      >>> c.publish_dataset(df, name='my_dataset')\n",
      " |      \n",
      " |      Receiving client:\n",
      " |      \n",
      " |      >>> c.list_datasets()  # doctest: +SKIP\n",
      " |      ['my_dataset']\n",
      " |      >>> df2 = c.get_dataset('my_dataset')  # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.list_datasets\n",
      " |      Client.get_dataset\n",
      " |      Client.unpublish_dataset\n",
      " |      Client.persist\n",
      " |  \n",
      " |  rebalance(self, futures=None, workers=None, **kwargs)\n",
      " |      Rebalance data within network\n",
      " |      \n",
      " |      Move data between workers to roughly balance memory burden.  This\n",
      " |      either affects a subset of the keys/workers or the entire network,\n",
      " |      depending on keyword arguments.\n",
      " |      \n",
      " |      For details on the algorithm and configuration options, refer to the matching\n",
      " |      scheduler-side method :meth:`~distributed.scheduler.Scheduler.rebalance`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |         This operation is generally not well tested against normal operation of the\n",
      " |         scheduler. It is not recommended to use it while waiting on computations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list, optional\n",
      " |          A list of futures to balance, defaults all data\n",
      " |      workers : list, optional\n",
      " |          A list of workers on which to balance, defaults to all workers\n",
      " |  \n",
      " |  register_scheduler_plugin(self, plugin, name=None, **kwargs)\n",
      " |      Register a scheduler plugin.\n",
      " |      \n",
      " |      See https://distributed.readthedocs.io/en/latest/plugins.html#scheduler-plugins\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      plugin : SchedulerPlugin\n",
      " |          Plugin class or object to pass to the scheduler.\n",
      " |      name : str\n",
      " |          Name for the plugin; if None, a name is taken from the\n",
      " |          plugin instance or automatically generated if not present.\n",
      " |      **kwargs : Any\n",
      " |          Arguments passed to the Plugin class (if Plugin is an\n",
      " |          instance kwargs are unused).\n",
      " |  \n",
      " |  register_worker_callbacks(self, setup=None)\n",
      " |      Registers a setup callback function for all current and future workers.\n",
      " |      \n",
      " |      This registers a new setup function for workers in this cluster. The\n",
      " |      function will run immediately on all currently connected workers. It\n",
      " |      will also be run upon connection by any workers that are added in the\n",
      " |      future. Multiple setup functions can be registered - these will be\n",
      " |      called in the order they were added.\n",
      " |      \n",
      " |      If the function takes an input argument named ``dask_worker`` then\n",
      " |      that variable will be populated with the worker itself.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      setup : callable(dask_worker: Worker) -> None\n",
      " |          Function to register and run on all workers\n",
      " |  \n",
      " |  register_worker_plugin(self, plugin=None, name=None, nanny=None, **kwargs)\n",
      " |      Registers a lifecycle worker plugin for all current and future workers.\n",
      " |      \n",
      " |      This registers a new object to handle setup, task state transitions and\n",
      " |      teardown for workers in this cluster. The plugin will instantiate itself\n",
      " |      on all currently connected workers. It will also be run on any worker\n",
      " |      that connects in the future.\n",
      " |      \n",
      " |      The plugin may include methods ``setup``, ``teardown``, ``transition``,\n",
      " |      and ``release_key``.  See the\n",
      " |      ``dask.distributed.WorkerPlugin`` class or the examples below for the\n",
      " |      interface and docstrings.  It must be serializable with the pickle or\n",
      " |      cloudpickle modules.\n",
      " |      \n",
      " |      If the plugin has a ``name`` attribute, or if the ``name=`` keyword is\n",
      " |      used then that will control idempotency.  If a plugin with that name has\n",
      " |      already been registered then any future plugins will not run.\n",
      " |      \n",
      " |      For alternatives to plugins, you may also wish to look into preload\n",
      " |      scripts.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      plugin : WorkerPlugin or NannyPlugin\n",
      " |          The plugin object to register.\n",
      " |      name : str, optional\n",
      " |          A name for the plugin.\n",
      " |          Registering a plugin with the same name will have no effect.\n",
      " |          If plugin has no name attribute a random name is used.\n",
      " |      nanny : bool, optional\n",
      " |          Whether to register the plugin with workers or nannies.\n",
      " |      **kwargs : optional\n",
      " |          If you pass a class as the plugin, instead of a class instance, then the\n",
      " |          class will be instantiated with any extra keyword arguments.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> class MyPlugin(WorkerPlugin):\n",
      " |      ...     def __init__(self, *args, **kwargs):\n",
      " |      ...         pass  # the constructor is up to you\n",
      " |      ...     def setup(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def teardown(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def transition(self, key: str, start: str, finish: str, **kwargs):\n",
      " |      ...         pass\n",
      " |      ...     def release_key(self, key: str, state: str, cause: Optional[str], reason: None, report: bool):\n",
      " |      ...         pass\n",
      " |      \n",
      " |      >>> plugin = MyPlugin(1, 2, 3)\n",
      " |      >>> client.register_worker_plugin(plugin)\n",
      " |      \n",
      " |      You can get access to the plugin with the ``get_worker`` function\n",
      " |      \n",
      " |      >>> client.register_worker_plugin(other_plugin, name='my-plugin')\n",
      " |      >>> def f():\n",
      " |      ...    worker = get_worker()\n",
      " |      ...    plugin = worker.plugins['my-plugin']\n",
      " |      ...    return plugin.my_state\n",
      " |      \n",
      " |      >>> future = client.run(f)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      distributed.WorkerPlugin\n",
      " |      unregister_worker_plugin\n",
      " |  \n",
      " |  replicate(self, futures, n=None, workers=None, branching_factor=2, **kwargs)\n",
      " |      Set replication of futures within network\n",
      " |      \n",
      " |      Copy data onto many workers.  This helps to broadcast frequently\n",
      " |      accessed data and it helps to improve resilience.\n",
      " |      \n",
      " |      This performs a tree copy of the data throughout the network\n",
      " |      individually on each piece of data.  This operation blocks until\n",
      " |      complete.  It does not guarantee replication of data to future workers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list of futures\n",
      " |          Futures we wish to replicate\n",
      " |      n : int, optional\n",
      " |          Number of processes on the cluster on which to replicate the data.\n",
      " |          Defaults to all.\n",
      " |      workers : list of worker addresses\n",
      " |          Workers on which we want to restrict the replication.\n",
      " |          Defaults to all.\n",
      " |      branching_factor : int, optional\n",
      " |          The number of workers that can copy data in each generation\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = c.submit(func, *args)  # doctest: +SKIP\n",
      " |      >>> c.replicate([x])  # send to all workers  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], n=3)  # send to three workers  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], workers=['alice', 'bob'])  # send to specific  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], n=1, workers=['alice', 'bob'])  # send to one of specific workers  # doctest: +SKIP\n",
      " |      >>> c.replicate([x], n=1)  # reduce replications # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.rebalance\n",
      " |  \n",
      " |  restart(self, **kwargs)\n",
      " |      Restart the distributed network\n",
      " |      \n",
      " |      This kills all active work, deletes all data on the network, and\n",
      " |      restarts the worker processes.\n",
      " |  \n",
      " |  retire_workers(self, workers=None, close_workers=True, **kwargs)\n",
      " |      Retire certain workers on the scheduler\n",
      " |      \n",
      " |      See dask.distributed.Scheduler.retire_workers for the full docstring.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      You can get information about active workers using the following:\n",
      " |      \n",
      " |      >>> workers = client.scheduler_info()['workers']\n",
      " |      \n",
      " |      From that list you may want to select some workers to close\n",
      " |      \n",
      " |      >>> client.retire_workers(workers=['tcp://address:port', ...])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.distributed.Scheduler.retire_workers\n",
      " |  \n",
      " |  retry(self, futures, asynchronous=None)\n",
      " |      Retry failed futures\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list of Futures\n",
      " |  \n",
      " |  run(self, function, *args, **kwargs)\n",
      " |      Run a function on all workers outside of task scheduling system\n",
      " |      \n",
      " |      This calls a function on all currently known workers immediately,\n",
      " |      blocks until those results come back, and returns the results\n",
      " |      asynchronously as a dictionary keyed by worker address.  This method\n",
      " |      if generally used for side effects, such and collecting diagnostic\n",
      " |      information or installing libraries.\n",
      " |      \n",
      " |      If your function takes an input argument named ``dask_worker`` then\n",
      " |      that variable will be populated with the worker itself.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      function : callable\n",
      " |      *args : arguments for remote function\n",
      " |      **kwargs : keyword arguments for remote function\n",
      " |      workers : list\n",
      " |          Workers on which to run the function. Defaults to all known workers.\n",
      " |      wait : boolean (optional)\n",
      " |          If the function is asynchronous whether or not to wait until that\n",
      " |          function finishes.\n",
      " |      nanny : bool, defualt False\n",
      " |          Whether to run ``function`` on the nanny. By default, the function\n",
      " |          is run on the worker process.  If specified, the addresses in\n",
      " |          ``workers`` should still be the worker addresses, not the nanny addresses.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.run(os.getpid)  # doctest: +SKIP\n",
      " |      {'192.168.0.100:9000': 1234,\n",
      " |       '192.168.0.101:9000': 4321,\n",
      " |       '192.168.0.102:9000': 5555}\n",
      " |      \n",
      " |      Restrict computation to particular workers with the ``workers=``\n",
      " |      keyword argument.\n",
      " |      \n",
      " |      >>> c.run(os.getpid, workers=['192.168.0.100:9000',\n",
      " |      ...                           '192.168.0.101:9000'])  # doctest: +SKIP\n",
      " |      {'192.168.0.100:9000': 1234,\n",
      " |       '192.168.0.101:9000': 4321}\n",
      " |      \n",
      " |      >>> def get_status(dask_worker):\n",
      " |      ...     return dask_worker.status\n",
      " |      \n",
      " |      >>> c.run(get_hostname)  # doctest: +SKIP\n",
      " |      {'192.168.0.100:9000': 'running',\n",
      " |       '192.168.0.101:9000': 'running}\n",
      " |      \n",
      " |      Run asynchronous functions in the background:\n",
      " |      \n",
      " |      >>> async def print_state(dask_worker):  # doctest: +SKIP\n",
      " |      ...    while True:\n",
      " |      ...        print(dask_worker.status)\n",
      " |      ...        await asyncio.sleep(1)\n",
      " |      \n",
      " |      >>> c.run(print_state, wait=False)  # doctest: +SKIP\n",
      " |  \n",
      " |  run_coroutine(self, function, *args, **kwargs)\n",
      " |      Spawn a coroutine on all workers.\n",
      " |      \n",
      " |      This spawns a coroutine on all currently known workers and then waits\n",
      " |      for the coroutine on each worker.  The coroutines' results are returned\n",
      " |      as a dictionary keyed by worker address.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      function : a coroutine function\n",
      " |          (typically a function wrapped in gen.coroutine or\n",
      " |           a Python 3.5+ async function)\n",
      " |      *args : arguments for remote function\n",
      " |      **kwargs : keyword arguments for remote function\n",
      " |      wait : boolean (default True)\n",
      " |          Whether to wait for coroutines to end.\n",
      " |      workers : list\n",
      " |          Workers on which to run the function. Defaults to all known workers.\n",
      " |  \n",
      " |  run_on_scheduler(self, function, *args, **kwargs)\n",
      " |      Run a function on the scheduler process\n",
      " |      \n",
      " |      This is typically used for live debugging.  The function should take a\n",
      " |      keyword argument ``dask_scheduler=``, which will be given the scheduler\n",
      " |      object itself.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def get_number_of_tasks(dask_scheduler=None):\n",
      " |      ...     return len(dask_scheduler.tasks)\n",
      " |      \n",
      " |      >>> client.run_on_scheduler(get_number_of_tasks)  # doctest: +SKIP\n",
      " |      100\n",
      " |      \n",
      " |      Run asynchronous functions in the background:\n",
      " |      \n",
      " |      >>> async def print_state(dask_scheduler):  # doctest: +SKIP\n",
      " |      ...    while True:\n",
      " |      ...        print(dask_scheduler.status)\n",
      " |      ...        await asyncio.sleep(1)\n",
      " |      \n",
      " |      >>> c.run(print_state, wait=False)  # doctest: +SKIP\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.run : Run a function on all workers\n",
      " |      Client.start_ipython_scheduler : Start an IPython session on scheduler\n",
      " |  \n",
      " |  scatter(self, data, workers=None, broadcast=False, direct=None, hash=True, timeout='__no_default__', asynchronous=None)\n",
      " |      Scatter data into distributed memory\n",
      " |      \n",
      " |      This moves data from the local client process into the workers of the\n",
      " |      distributed scheduler.  Note that it is often better to submit jobs to\n",
      " |      your workers to have them load the data rather than loading data\n",
      " |      locally and then scattering it out to them.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : list, dict, or object\n",
      " |          Data to scatter out to workers.  Output type matches input type.\n",
      " |      workers : list of tuples (optional)\n",
      " |          Optionally constrain locations of data.\n",
      " |          Specify workers as hostname/port pairs, e.g. ``('127.0.0.1', 8787)``.\n",
      " |      broadcast : bool (defaults to False)\n",
      " |          Whether to send each data element to all workers.\n",
      " |          By default we round-robin based on number of cores.\n",
      " |      direct : bool (defaults to automatically check)\n",
      " |          Whether or not to connect directly to the workers, or to ask\n",
      " |          the scheduler to serve as intermediary.  This can also be set when\n",
      " |          creating the Client.\n",
      " |      hash : bool (optional)\n",
      " |          Whether or not to hash data to determine key.\n",
      " |          If False then this uses a random key\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List, dict, iterator, or queue of futures matching the type of input.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c = Client('127.0.0.1:8787')  # doctest: +SKIP\n",
      " |      >>> c.scatter(1) # doctest: +SKIP\n",
      " |      <Future: status: finished, key: c0a8a20f903a4915b94db8de3ea63195>\n",
      " |      \n",
      " |      >>> c.scatter([1, 2, 3])  # doctest: +SKIP\n",
      " |      [<Future: status: finished, key: c0a8a20f903a4915b94db8de3ea63195>,\n",
      " |       <Future: status: finished, key: 58e78e1b34eb49a68c65b54815d1b158>,\n",
      " |       <Future: status: finished, key: d3395e15f605bc35ab1bac6341a285e2>]\n",
      " |      \n",
      " |      >>> c.scatter({'x': 1, 'y': 2, 'z': 3})  # doctest: +SKIP\n",
      " |      {'x': <Future: status: finished, key: x>,\n",
      " |       'y': <Future: status: finished, key: y>,\n",
      " |       'z': <Future: status: finished, key: z>}\n",
      " |      \n",
      " |      Constrain location of data to subset of workers\n",
      " |      \n",
      " |      >>> c.scatter([1, 2, 3], workers=[('hostname', 8788)])   # doctest: +SKIP\n",
      " |      \n",
      " |      Broadcast data to all workers\n",
      " |      \n",
      " |      >>> [future] = c.scatter([element], broadcast=True)  # doctest: +SKIP\n",
      " |      \n",
      " |      Send scattered data to parallelized function using client futures\n",
      " |      interface\n",
      " |      \n",
      " |      >>> data = c.scatter(data, broadcast=True)  # doctest: +SKIP\n",
      " |      >>> res = [c.submit(func, data, i) for i in range(100)]\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.gather : Gather data back to local process\n",
      " |  \n",
      " |  scheduler_info(self, **kwargs)\n",
      " |      Basic information about the workers in the cluster\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.scheduler_info()  # doctest: +SKIP\n",
      " |      {'id': '2de2b6da-69ee-11e6-ab6a-e82aea155996',\n",
      " |       'services': {},\n",
      " |       'type': 'Scheduler',\n",
      " |       'workers': {'127.0.0.1:40575': {'active': 0,\n",
      " |                                       'last-seen': 1472038237.4845693,\n",
      " |                                       'name': '127.0.0.1:40575',\n",
      " |                                       'services': {},\n",
      " |                                       'stored': 0,\n",
      " |                                       'time-delay': 0.0061032772064208984}}}\n",
      " |  \n",
      " |  set_metadata(self, key, value)\n",
      " |      Set arbitrary metadata in the scheduler\n",
      " |      \n",
      " |      This allows you to store small amounts of data on the central scheduler\n",
      " |      process for administrative purposes.  Data should be msgpack\n",
      " |      serializable (ints, strings, lists, dicts)\n",
      " |      \n",
      " |      If the key corresponds to a task then that key will be cleaned up when\n",
      " |      the task is forgotten by the scheduler.\n",
      " |      \n",
      " |      If the key is a list then it will be assumed that you want to index\n",
      " |      into a nested dictionary structure using those keys.  For example if\n",
      " |      you call the following::\n",
      " |      \n",
      " |          >>> client.set_metadata(['a', 'b', 'c'], 123)\n",
      " |      \n",
      " |      Then this is the same as setting\n",
      " |      \n",
      " |          >>> scheduler.task_metadata['a']['b']['c'] = 123\n",
      " |      \n",
      " |      The lower level dictionaries will be created on demand.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.set_metadata('x', 123)  # doctest: +SKIP\n",
      " |      >>> client.get_metadata('x')  # doctest: +SKIP\n",
      " |      123\n",
      " |      \n",
      " |      >>> client.set_metadata(['x', 'y'], 123)  # doctest: +SKIP\n",
      " |      >>> client.get_metadata('x')  # doctest: +SKIP\n",
      " |      {'y': 123}\n",
      " |      \n",
      " |      >>> client.set_metadata(['x', 'w', 'z'], 456)  # doctest: +SKIP\n",
      " |      >>> client.get_metadata('x')  # doctest: +SKIP\n",
      " |      {'y': 123, 'w': {'z': 456}}\n",
      " |      \n",
      " |      >>> client.get_metadata(['x', 'w'])  # doctest: +SKIP\n",
      " |      {'z': 456}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_metadata\n",
      " |  \n",
      " |  shutdown(self)\n",
      " |      Shut down the connected scheduler and workers\n",
      " |      \n",
      " |      Note, this may disrupt other clients that may be using the same\n",
      " |      scheduler and workers.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.close : close only this client\n",
      " |  \n",
      " |  start(self, **kwargs)\n",
      " |      Start scheduler running in separate thread\n",
      " |  \n",
      " |  start_ipython(self, *args, **kwargs)\n",
      " |      Deprecated - Method moved to start_ipython_workers\n",
      " |  \n",
      " |  start_ipython_scheduler(self, magic_name='scheduler_if_ipython', qtconsole=False, qtconsole_args=None)\n",
      " |      Start IPython kernel on the scheduler\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      magic_name : str or None (optional)\n",
      " |          If defined, register IPython magic with this name for\n",
      " |          executing code on the scheduler.\n",
      " |          If not defined, register %scheduler magic if IPython is running.\n",
      " |      qtconsole : bool (optional)\n",
      " |          If True, launch a Jupyter QtConsole connected to the worker(s).\n",
      " |      qtconsole_args : list(str) (optional)\n",
      " |          Additional arguments to pass to the qtconsole on startup.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.start_ipython_scheduler() # doctest: +SKIP\n",
      " |      >>> %scheduler scheduler.processing  # doctest: +SKIP\n",
      " |      {'127.0.0.1:3595': {'inc-1', 'inc-2'},\n",
      " |       '127.0.0.1:53589': {'inc-2', 'add-5'}}\n",
      " |      \n",
      " |      >>> c.start_ipython_scheduler(qtconsole=True) # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      connection_info: dict\n",
      " |          connection_info dict containing info necessary\n",
      " |          to connect Jupyter clients to the scheduler.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.start_ipython_workers : Start IPython on the workers\n",
      " |  \n",
      " |  start_ipython_workers(self, workers=None, magic_names=False, qtconsole=False, qtconsole_args=None)\n",
      " |      Start IPython kernels on workers\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      workers : list (optional)\n",
      " |          A list of worker addresses, defaults to all\n",
      " |      magic_names : str or list(str) (optional)\n",
      " |          If defined, register IPython magics with these names for\n",
      " |          executing code on the workers.  If string has asterix then expand\n",
      " |          asterix into 0, 1, ..., n for n workers\n",
      " |      qtconsole : bool (optional)\n",
      " |          If True, launch a Jupyter QtConsole connected to the worker(s).\n",
      " |      qtconsole_args : list(str) (optional)\n",
      " |          Additional arguments to pass to the qtconsole on startup.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> info = c.start_ipython_workers() # doctest: +SKIP\n",
      " |      >>> %remote info['192.168.1.101:5752'] worker.data  # doctest: +SKIP\n",
      " |      {'x': 1, 'y': 100}\n",
      " |      \n",
      " |      >>> c.start_ipython_workers('192.168.1.101:5752', magic_names='w') # doctest: +SKIP\n",
      " |      >>> %w worker.data  # doctest: +SKIP\n",
      " |      {'x': 1, 'y': 100}\n",
      " |      \n",
      " |      >>> c.start_ipython_workers('192.168.1.101:5752', qtconsole=True) # doctest: +SKIP\n",
      " |      \n",
      " |      Add asterix * in magic names to add one magic per worker\n",
      " |      \n",
      " |      >>> c.start_ipython_workers(magic_names='w_*') # doctest: +SKIP\n",
      " |      >>> %w_0 worker.data  # doctest: +SKIP\n",
      " |      {'x': 1, 'y': 100}\n",
      " |      >>> %w_1 worker.data  # doctest: +SKIP\n",
      " |      {'z': 5}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      iter_connection_info: list\n",
      " |          List of connection_info dicts containing info necessary\n",
      " |          to connect Jupyter clients to the workers.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.start_ipython_scheduler : start ipython on the scheduler\n",
      " |  \n",
      " |  submit(self, func, *args, key=None, workers=None, resources=None, retries=None, priority=0, fifo_timeout='100 ms', allow_other_workers=False, actor=False, actors=False, pure=None, **kwargs)\n",
      " |      Submit a function application to the scheduler\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Callable to be scheduled as ``func(*args **kwargs)``. If ``func`` returns a\n",
      " |          coroutine, it will be run on the main event loop of a worker. Otherwise\n",
      " |          ``func`` will be run in a worker's task executor pool (see\n",
      " |          ``Worker.executors`` for more information.)\n",
      " |      *args\n",
      " |      **kwargs\n",
      " |      pure : bool (defaults to True)\n",
      " |          Whether or not the function is pure.  Set ``pure=False`` for\n",
      " |          impure functions like ``np.random.random``.\n",
      " |          See :ref:`pure functions` for more details.\n",
      " |      workers : string or iterable of strings\n",
      " |          A set of worker addresses or hostnames on which computations may be\n",
      " |          performed. Leave empty to default to all workers (common case)\n",
      " |      key : str\n",
      " |          Unique identifier for the task.  Defaults to function-name and hash\n",
      " |      allow_other_workers : bool (defaults to False)\n",
      " |          Used with ``workers``. Indicates whether or not the computations\n",
      " |          may be performed on workers that are not in the `workers` set(s).\n",
      " |      retries : int (default to 0)\n",
      " |          Number of allowed automatic retries if the task fails\n",
      " |      priority : Number\n",
      " |          Optional prioritization of task.  Zero is default.\n",
      " |          Higher priorities take precedence\n",
      " |      fifo_timeout : str timedelta (default '100ms')\n",
      " |          Allowed amount of time between calls to consider the same priority\n",
      " |      resources : dict (defaults to {})\n",
      " |          Defines the ``resources`` each instance of this mapped task requires\n",
      " |          on the worker; e.g. ``{'GPU': 2}``.\n",
      " |          See :doc:`worker resources <resources>` for details on defining\n",
      " |          resources.\n",
      " |      actor : bool (default False)\n",
      " |          Whether this task should exist on the worker as a stateful actor.\n",
      " |          See :doc:`actors` for additional details.\n",
      " |      actors : bool (default False)\n",
      " |          Alias for `actor`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c = client.submit(add, a, b)  # doctest: +SKIP\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Future\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.map : Submit on many arguments at once\n",
      " |  \n",
      " |  sync(self, func, *args, asynchronous=None, callback_timeout=None, **kwargs)\n",
      " |  \n",
      " |  unpublish_dataset(self, name, **kwargs)\n",
      " |      Remove named datasets from scheduler\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> c.list_datasets()  # doctest: +SKIP\n",
      " |      ['my_dataset']\n",
      " |      >>> c.unpublish_datasets('my_dataset')  # doctest: +SKIP\n",
      " |      >>> c.list_datasets()  # doctest: +SKIP\n",
      " |      []\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.publish_dataset\n",
      " |  \n",
      " |  unregister_worker_plugin(self, name, nanny=None)\n",
      " |      Unregisters a lifecycle worker plugin\n",
      " |      \n",
      " |      This unregisters an existing worker plugin. As part of the unregistration process\n",
      " |      the plugin's ``teardown`` method will be called.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the plugin to unregister. See the :meth:`Client.register_worker_plugin`\n",
      " |          docstring for more information.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> class MyPlugin(WorkerPlugin):\n",
      " |      ...     def __init__(self, *args, **kwargs):\n",
      " |      ...         pass  # the constructor is up to you\n",
      " |      ...     def setup(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def teardown(self, worker: dask.distributed.Worker):\n",
      " |      ...         pass\n",
      " |      ...     def transition(self, key: str, start: str, finish: str, **kwargs):\n",
      " |      ...         pass\n",
      " |      ...     def release_key(self, key: str, state: str, cause: Optional[str], reason: None, report: bool):\n",
      " |      ...         pass\n",
      " |      \n",
      " |      >>> plugin = MyPlugin(1, 2, 3)\n",
      " |      >>> client.register_worker_plugin(plugin, name='foo')\n",
      " |      >>> client.unregister_worker_plugin(name='foo')\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      register_worker_plugin\n",
      " |  \n",
      " |  upload_file(self, filename, **kwargs)\n",
      " |      Upload local package to workers\n",
      " |      \n",
      " |      This sends a local file up to all worker nodes.  This file is placed\n",
      " |      into a temporary directory on Python's system path so any .py,  .egg\n",
      " |      or .zip  files will be importable.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : string\n",
      " |          Filename of .py, .egg or .zip file to send to workers\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client.upload_file('mylibrary.egg')  # doctest: +SKIP\n",
      " |      >>> from mylibrary import myfunc  # doctest: +SKIP\n",
      " |      >>> L = client.map(myfunc, seq)  # doctest: +SKIP\n",
      " |  \n",
      " |  wait_for_workers(self, n_workers=0, timeout=None)\n",
      " |      Blocking call to wait for n workers before continuing\n",
      " |  \n",
      " |  who_has(self, futures=None, **kwargs)\n",
      " |      The workers storing each future's data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      futures : list (optional)\n",
      " |          A list of futures, defaults to all data\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x, y, z = c.map(inc, [1, 2, 3])  # doctest: +SKIP\n",
      " |      >>> wait([x, y, z])  # doctest: +SKIP\n",
      " |      >>> c.who_has()  # doctest: +SKIP\n",
      " |      {'inc-1c8dd6be1c21646c71f76c16d09304ea': ['192.168.1.141:46784'],\n",
      " |       'inc-1e297fc27658d7b67b3a758f16bcf47a': ['192.168.1.141:46784'],\n",
      " |       'inc-fd65c238a7ea60f6a01bf4c8a5fcf44b': ['192.168.1.141:46784']}\n",
      " |      \n",
      " |      >>> c.who_has([x, y])  # doctest: +SKIP\n",
      " |      {'inc-1c8dd6be1c21646c71f76c16d09304ea': ['192.168.1.141:46784'],\n",
      " |       'inc-1e297fc27658d7b67b3a758f16bcf47a': ['192.168.1.141:46784']}\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Client.has_what\n",
      " |      Client.nthreads\n",
      " |  \n",
      " |  write_scheduler_file(self, scheduler_file)\n",
      " |      Write the scheduler information to a json file.\n",
      " |      \n",
      " |      This facilitates easy sharing of scheduler information using a file\n",
      " |      system. The scheduler file can be used to instantiate a second Client\n",
      " |      using the same scheduler.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      scheduler_file : str\n",
      " |          Path to a write the scheduler file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> client = Client()  # doctest: +SKIP\n",
      " |      >>> client.write_scheduler_file('scheduler.json')  # doctest: +SKIP\n",
      " |      # connect to previous client's scheduler\n",
      " |      >>> client2 = Client(scheduler_file='scheduler.json')  # doctest: +SKIP\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  current(allow_global=True) from builtins.type\n",
      " |      When running within the context of `as_client`, return the context-local\n",
      " |      current client. Otherwise, return the latest initialised Client.\n",
      " |      If no Client instances exist, raise ValueError.\n",
      " |      If allow_global is set to False, raise ValueError if running outside of the\n",
      " |      `as_client` context manager.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  collections_to_dsk(collections, *args, **kwargs)\n",
      " |      Convert many collections into a single dask graph, after optimization\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  asynchronous\n",
      " |      Are we running in the event loop?\n",
      " |      \n",
      " |      This is true if the user signaled that we might be when creating the\n",
      " |      client as in the following::\n",
      " |      \n",
      " |          client = Client(asynchronous=True)\n",
      " |      \n",
      " |      However, we override this expectation if we can definitively tell that\n",
      " |      we are running from a thread that is not the event loop.  This is\n",
      " |      common when calling get_client() from within a worker task.  Even\n",
      " |      though the client was originally created in asynchronous mode we may\n",
      " |      find ourselves in contexts when it is better to operate synchronously.\n",
      " |  \n",
      " |  dashboard_link\n",
      " |      Link to the scheduler's dashboard.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Dashboard URL.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Opening the dashboard in your default web browser:\n",
      " |      \n",
      " |      >>> import webbrowser\n",
      " |      >>> from distributed import Client\n",
      " |      >>> client = Client()\n",
      " |      >>> webbrowser.open(client.dashboard_link)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188d2ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 762.94 MiB </td>\n",
       "                        <td> 7.63 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (10000, 10000) </td>\n",
       "                        <td> (1000, 1000) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 100 Tasks </td>\n",
       "                        <td> 100 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"120\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"120\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"120\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"120\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"120\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"120\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"84\" x2=\"120\" y2=\"84\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"120\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"108\" x2=\"120\" y2=\"108\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"120\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"120\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"120\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"120\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"120\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"120\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"120\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"120\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"120\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10000</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,60.000000)\">10000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525a2640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table>\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 39.06 kiB </td>\n",
       "                        <td> 3.91 kiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (5000,) </td>\n",
       "                        <td> (500,) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Count </th>\n",
       "                        <td> 430 Tasks </td>\n",
       "                        <td> 10 Chunks </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                    <th> Type </th>\n",
       "                    <td> float64 </td>\n",
       "                    <td> numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"25\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"25\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"25\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"25\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"25\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >5000</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<mean_agg-aggregate, shape=(5000,), dtype=float64, chunksize=(500,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd6fc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0005167 , 0.99342854, 1.002812  , ..., 1.00121189, 0.99755443,\n",
       "       0.99562434])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33054a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
